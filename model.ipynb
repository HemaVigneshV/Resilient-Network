{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_csv_train shape: (2262300, 7)\n",
      "X_cicids_train shape: (2262300, 78)\n",
      "y_cicids_train shape: (2262300,)\n",
      "Epoch 1/10\n",
      "\u001b[1m56558/56558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 3ms/step - accuracy: 0.2527 - loss: -145214423040.0000 - val_accuracy: 0.0729 - val_loss: -1552586768384.0000\n",
      "Epoch 2/10\n",
      "\u001b[1m56558/56558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 3ms/step - accuracy: 0.2581 - loss: -7787124883456.0000 - val_accuracy: 0.0430 - val_loss: -20391265304576.0000\n",
      "Epoch 3/10\n",
      "\u001b[1m56558/56558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 3ms/step - accuracy: 0.2589 - loss: -61957403049984.0000 - val_accuracy: 0.0776 - val_loss: 2426378423107584.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m56558/56558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 3ms/step - accuracy: 0.2640 - loss: -243487232491520.0000 - val_accuracy: 0.0737 - val_loss: 1589964278071296.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m56558/56558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 3ms/step - accuracy: 0.2650 - loss: -682504998617088.0000 - val_accuracy: 0.0903 - val_loss: 13121858454945792.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m56558/56558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 3ms/step - accuracy: 0.2641 - loss: -1527817980346368.0000 - val_accuracy: 0.0895 - val_loss: 12748066377433088.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m56558/56558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 3ms/step - accuracy: 0.2671 - loss: -3021400507940864.0000 - val_accuracy: 0.0591 - val_loss: 21931926226993152.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m56558/56558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 3ms/step - accuracy: 0.2670 - loss: -5380431696363520.0000 - val_accuracy: 0.0899 - val_loss: 4894804576043008.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m56558/56558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 3ms/step - accuracy: 0.2655 - loss: -8887964520153088.0000 - val_accuracy: 0.2003 - val_loss: 18280978539610112.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m56558/56558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 3ms/step - accuracy: 0.2656 - loss: -13886857628614656.0000 - val_accuracy: 0.0790 - val_loss: 560531749394710528.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17675/17675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2ms/step - accuracy: 0.0794 - loss: 535270229228388352.0000\n",
      "Test Loss: 4.20437990737707e+17\n",
      "Test Accuracy: 0.07966037839651108\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Concatenate\n",
    "\n",
    "# Paths to datasets\n",
    "csv_path = 'csv/'\n",
    "cicids_path = 'cicids/'\n",
    "\n",
    "# Function to load CSV files from a folder and merge them into a single DataFrame\n",
    "def load_and_merge_csv(folder_path):\n",
    "    all_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "    dataframes = [pd.read_csv(file) for file in all_files]\n",
    "    return pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Load datasets\n",
    "csv_data = load_and_merge_csv(csv_path)\n",
    "cicids_data = load_and_merge_csv(cicids_path)\n",
    "\n",
    "# Preprocessing for CSV dataset\n",
    "def preprocess_csv_data(data):\n",
    "    # Standardize numerical fields\n",
    "    scaler = StandardScaler()\n",
    "    numerical_cols = ['time', 'data_len', 'src_port', 'dst_port']\n",
    "    data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "\n",
    "    # Encode protocol field\n",
    "    data['proto'] = LabelEncoder().fit_transform(data['proto'])\n",
    "\n",
    "    # Convert IP addresses to numerical representations\n",
    "    data['ip_src'] = data['ip_src'].apply(lambda x: int.from_bytes(map(int, x.split('.')), 'big'))\n",
    "    data['ip_dst'] = data['ip_dst'].apply(lambda x: int.from_bytes(map(int, x.split('.')), 'big'))\n",
    "\n",
    "    return data\n",
    "\n",
    "# Preprocessing for CICIDS dataset\n",
    "def preprocess_cicids_data(data):\n",
    "    # Drop rows with any missing or infinite values\n",
    "    data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    # Separate features and labels\n",
    "    labels = data[' Label']  # Extract labels\n",
    "    features = data.drop(columns=[' Label'])  # Remove label column from features\n",
    "\n",
    "    # Ensure all features are numeric\n",
    "    features = features.apply(pd.to_numeric, errors='coerce')  # Convert non-numeric values\n",
    "    features.fillna(0, inplace=True)  # Replace remaining NaNs with 0\n",
    "\n",
    "    # Encode labels (BENIGN = 0, others = 1)\n",
    "    labels = LabelEncoder().fit_transform(labels)\n",
    "\n",
    "    # Standardize numerical fields\n",
    "    features = StandardScaler().fit_transform(features)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "# Model creation\n",
    "def build_model(csv_input_shape, cicids_input_shape):\n",
    "    # Input layers\n",
    "    csv_input = Input(shape=(csv_input_shape,), name=\"csv_input\")\n",
    "    cicids_input = Input(shape=(cicids_input_shape,), name=\"cicids_input\")\n",
    "\n",
    "    # CSV branch\n",
    "    csv_branch = Dense(64, activation='relu')(csv_input)\n",
    "    csv_branch = BatchNormalization()(csv_branch)\n",
    "    csv_branch = Dropout(0.3)(csv_branch)\n",
    "\n",
    "    # CICIDS branch\n",
    "    cicids_branch = Dense(64, activation='relu')(cicids_input)\n",
    "    cicids_branch = BatchNormalization()(cicids_branch)\n",
    "    cicids_branch = Dropout(0.3)(cicids_branch)\n",
    "\n",
    "    # Concatenate branches\n",
    "    merged = Concatenate()([csv_branch, cicids_branch])\n",
    "    merged = Dense(128, activation='relu')(merged)\n",
    "    merged = Dropout(0.4)(merged)\n",
    "    merged = Dense(64, activation='relu')(merged)\n",
    "\n",
    "    # Output layer\n",
    "    output = Dense(1, activation='sigmoid', name=\"output\")(merged)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs=[csv_input, cicids_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Preprocess both datasets\n",
    "csv_data = preprocess_csv_data(csv_data)  # Returns a DataFrame\n",
    "cicids_features, cicids_labels = preprocess_cicids_data(cicids_data)  # Features and labels\n",
    "\n",
    "# Ensure both datasets have the same number of samples\n",
    "min_samples = min(len(csv_data), len(cicids_features))\n",
    "csv_data = csv_data.iloc[:min_samples]\n",
    "cicids_features = cicids_features[:min_samples]\n",
    "cicids_labels = cicids_labels[:min_samples]\n",
    "\n",
    "# Split datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split CSV data into training and testing sets\n",
    "X_csv_train, X_csv_test = train_test_split(csv_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split CICIDS data into training and testing sets\n",
    "X_cicids_train, X_cicids_test, y_cicids_train, y_cicids_test = train_test_split(\n",
    "    cicids_features, cicids_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Check if the number of samples match\n",
    "print(\"X_csv_train shape:\", X_csv_train.shape)\n",
    "print(\"X_cicids_train shape:\", X_cicids_train.shape)\n",
    "print(\"y_cicids_train shape:\", y_cicids_train.shape)\n",
    "\n",
    "# Ensure the number of samples match between X_csv_train, X_cicids_train, and y_cicids_train\n",
    "assert X_csv_train.shape[0] == X_cicids_train.shape[0] == y_cicids_train.shape[0], \"Mismatched sample sizes!\"\n",
    "\n",
    "# Build the model\n",
    "model = build_model(X_csv_train.shape[1], X_cicids_train.shape[1])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [X_csv_train, X_cicids_train], y_cicids_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('self_healing_network_model.h5')\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation = model.evaluate([X_csv_test, X_cicids_test], y_cicids_test, verbose=1)\n",
    "\n",
    "print(\"Test Loss:\", evaluation[0])\n",
    "print(\"Test Accuracy:\", evaluation[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File renamed to: self_healing_network_model.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to the .h5 file\n",
    "h5_file_path = 'self_healing_network_model.keras'\n",
    "\n",
    "# Define the new .keras file path\n",
    "keras_file_path = 'self_healing_network_model.h5'\n",
    "\n",
    "# Rename the file\n",
    "os.rename(h5_file_path, keras_file_path)\n",
    "\n",
    "print(f\"File renamed to: {keras_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
